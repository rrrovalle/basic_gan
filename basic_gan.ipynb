{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOuqGVanGQtXlVYTwqXU8JZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrrovalle/basic_gan/blob/main/basic_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKlFjH1O9JvU"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "from keras.layers.advanced_activations import LeakyReLU\r\n",
        "from keras.models import Sequential, Model\r\n",
        "from keras.optimizers import Adam\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "x,y = mnist.load_data()  \r\n",
        "img_rows  = 28\r\n",
        "img_cols  = 28\r\n",
        "channels  = 1\r\n",
        "img_shape = (img_rows, img_cols, channels) \r\n",
        "\r\n",
        "def build_generator():\r\n",
        "  \r\n",
        "  noise_shape = (100,) #1D array of size 100(latentvector/noise)\r\n",
        "\r\n",
        "  model = Sequential()\r\n",
        "  # First Layer\r\n",
        "  model.add(Dense(256,input_shape=noise_shape))\r\n",
        "  model.add(LeakyReLU(alpha=0.2))\r\n",
        "  model.add(BatchNormalization(momentum=0.8))\r\n",
        "  # Second Layer\r\n",
        "  model.add(Dense(512))\r\n",
        "  model.add(LeakyReLU(alpha=0.2))\r\n",
        "  model.add(BatchNormalization(momentum=0.8))\r\n",
        "  # Third Layer\r\n",
        "  model.add(Dense(1024))\r\n",
        "  model.add(LeakyReLU(alpha=0.2))\r\n",
        "  model.add(BatchNormalization(momentum=0.8))\r\n",
        "\r\n",
        "  model.add(Dense(np.prod(img_shape), activation='tanh'))\r\n",
        "  model.add(Reshape(img_shape))\r\n",
        "\r\n",
        "  model.summary()\r\n",
        "  \r\n",
        "  noise = Input(shape = noise_shape)\r\n",
        "  img   = model(noise) # Generated image\r\n",
        "\r\n",
        "  return Model(noise, img)\r\n",
        "\r\n",
        "\r\n",
        "def build_discriminator():\r\n",
        "  \r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(Flatten(input_shape = img_shape))\r\n",
        "  model.add(Dense(512)) \r\n",
        "  model.add(LeakyReLU(alpha=0.2)) \r\n",
        "  model.add(Dense(256)) \r\n",
        "  model.add(LeakyReLU(alpha=0.2)) \r\n",
        "  model.add(Dense(1,activation='sigmoid'))\r\n",
        "\r\n",
        "  img      = Input(shape = img_shape)\r\n",
        "  validity = model(img)\r\n",
        "  \r\n",
        "  return Model(img, validity) # Validity is the discriminator's guess of input being real or not.\r\n",
        "\r\n",
        "\r\n",
        "def train(epochs, batch_size=128, save_interval=50):\r\n",
        "\r\n",
        "  #Load the dataset\r\n",
        "  (X_train, _), (_, _) = mnist.load_data()\r\n",
        "\r\n",
        "  # Convert to float nd Rescale -1 to 1 \r\n",
        "  X_train = (X_train.astype(np.float32) - 127.5) / 127.5 \r\n",
        "\r\n",
        "  # Add channels dimension. As the input to out gen and disc. has shape 28x28x1\r\n",
        "  X_train = np.expand_dims(X_train, axis=3)\r\n",
        "\r\n",
        "  half_batch = int(batch_size/2)\r\n",
        "\r\n",
        "  for epoch in range(epochs):\r\n",
        "    # Train Discriminator\r\n",
        "\r\n",
        "    # Select a random half batch of real images\r\n",
        "    idx  = np.random.randint(0, X_train.shape[0], half_batch)\r\n",
        "    imgs = X_train[idx]\r\n",
        "\r\n",
        "    noise = np.random.normal(0,1,(half_batch,100))\r\n",
        "\r\n",
        "    gen_imgs = generator.predict(noise)\r\n",
        "\r\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\r\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch,1)))\r\n",
        "\r\n",
        "    # take avera loss from real and fake images\r\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\r\n",
        "\r\n",
        "    # Train Generator\r\n",
        "    noise = np.random.normal(0,1,(batch_size, 100))\r\n",
        "\r\n",
        "    # Generator wants the discriminator to label the generated samples as valid ones\r\n",
        "    # this is where the generator is trying to trick discriminator into believing (hence value of 1 for y)\r\n",
        "    valid_y = np.array([1]*batch_size) #create an array of all ones of size-batch size\r\n",
        "    \r\n",
        "    # Generator is part of combined where it got directly linked with the disc.\r\n",
        "    #  train the generator with noise as x and 1 as y. Again, 1 as the output as it is \r\n",
        "    #  adversarial and if generator did a great job of folling the disc. the output would be 1 (true) \r\n",
        "    g_loss = combined.train_on_batch(noise, valid_y)\r\n",
        "\r\n",
        "\r\n",
        "    print(\"%d [D loss: %f, acc.: %.2f%% [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1],g_loss))\r\n",
        "\r\n",
        "    #if at save interval -> save generated image samples\r\n",
        "    if epoch % save_interval == 0:\r\n",
        "      save_imgs(epoch)\r\n",
        "\r\n",
        "\r\n",
        "def save_imgs(epoch):\r\n",
        "  r,c = 5,5\r\n",
        "  noise = np.random.normal(0,1,(r*c,100))\r\n",
        "  gen_imgs = generator.predict(noise)\r\n",
        "  \r\n",
        "  # rescale images 0-1\r\n",
        "  gen_imgs = 0.5 * gen_imgs + 0.5\r\n",
        "\r\n",
        "  fig, axs = plt.subplots(r,c)\r\n",
        "  cnt = 0\r\n",
        "  for i in range(r):\r\n",
        "    for j in range(c):\r\n",
        "      axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\r\n",
        "      axs[i,j].axis('off')\r\n",
        "      cnt += 1\r\n",
        "  fig.savefig(\"images/mnits_%d.png\" % epoch)\r\n",
        "  plt.close()\r\n",
        "\r\n",
        "\r\n",
        "optimizer = Adam(0.0002,0.5)\r\n",
        "discriminator = build_discriminator()\r\n",
        "discriminator.compile(loss='binary_crossentropy',\r\n",
        "                      optimizer=optimizer,\r\n",
        "                      metrics=['accuracy'])\r\n",
        "\r\n",
        "generator = build_generator()\r\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\r\n",
        "\r\n",
        "z   = Input(shape=(100,))\r\n",
        "img = generator(z)\r\n",
        "\r\n",
        "discriminator.trainable = False\r\n",
        "valid = discriminator(img)\r\n",
        "\r\n",
        "combined = Model(z, valid)\r\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\r\n",
        "train(epochs=30000, batch_size=32, save_interval=200)\r\n",
        "\r\n",
        "generator.save('generator_model_test.h5')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}